% aspectratio: 43 for old style 4:3, 169 for wide screen 16:9
% If you don't know if the screen will be wide screen or 4:3 you can use
% 1610 for a compromise to have smaller borders on either of the usual aspect ratios
% \documentclass[aspectratio=169,9pt]{beamer}
\documentclass[
aspectratio=169,
14pt,
professionalfonts
]{beamer}

\usetheme[patchframe=true]{LMU}
% \usepackage{fontspec}
% \setsansfont{Source Sans Pro}
% \setmonofont{Fira Code}[Contextuals=Alternate]

% \usepackage{mathspec}

\usepackage{float,subcaption}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\newcommand\identity{1\kern-0.25em\text{l}}

\newcommand{\pyhf}{\texttt{pyhf}\xspace}

\newcommand{\arrow}{~\ding{220}~}

\DeclareMathOperator\supp{supp}

\title[]{Illuminating the dark side of statistics: \\ Bayesian inference in particle physics}

\author[L. G\"artner]{\underline{Lorenz G\"artner}$^{1}$}

\institute[LMU]{$^1$LMU Munich}

\date{\today}

\begin{document}
%TODO references

\begin{frame}[titleslide]
    \titlepage
    \begin{tikzpicture}[remember picture,overlay]
    \node[anchor=south west, yshift=2mm, xshift=2mm] at (current page.south west)
    {%
      \includegraphics[height=1.cm]{common/logos/origins.pdf}
      \includegraphics[height=1.cm]{common/logos/belle2.pdf}
      \includegraphics[height=1.cm]{common/logos/punch4nfdi.png}
      \includegraphics[height=1.cm]{common/logos/dfg.jpg}
      % \includegraphics[height=1.5cm]{common/logos/bmbf.pdf}
    };%
    \end{tikzpicture}
\end{frame}

\begin{frame}{About me}
    \begin{minipage}{0.69\textwidth}
        \begin{itemize}
            \item BSc @ University of Manchester \\
                Physics with theoretical physics\\
            \item MSc @ LMU Munich \\
                More theory \ldots
        \end{itemize}
        $---$ almost no stats $---$
        \begin{itemize}
            \item Currenly PhD @ LMU Munich\\
                A lot of stats
        \end{itemize}
        \arrow Never too late to start
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \begin{figure}
            \center
            \includegraphics[width=\textwidth]{../plots/port.jpg}
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}
\centering
\Large
What is a probability?
\end{frame}

\begin{frame}{Kolmogorov probability axioms}
% https://plato.stanford.edu/entries/probability-interpret/#MaiInt
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{../plots/kolmogorov.jpg}
\end{figure}
\begin{enumerate}
  \item $ p(\Omega) = 1 $, where \( \Omega \) is the sample space.
  \item $ p(x) \geq 0 $ for any event \( x \subseteq \Omega \).
  \item For any sequence of disjoint events \( x_1, x_2, \dots \),
        $$
        p\left( \bigcup_{i=1} x_i \right) = \sum_{i=1} p(x_i)
        $$
\end{enumerate}
%https://www.urbanomic.com/document/concept-intuition-abstract-probability-theory/kolmogorov/
\end{frame}


% \begin{frame}{Conditional probability}
% Is \textbf{defined} as the probability of an event $x$ if we know that an event $y$ is true $p(x|y)$.
% $$p(x \cap y) = p(x|y)p(y) \quad \to \quad p(x|y) = \frac{p(x \cap y)}{p(y)}$$

% \textit{Note} 
% $$p(x \cap y) = p(y \cap x) \quad  \text{but} \quad p(x|y) \neq p(x|y)$$ 
% \end{frame}

\begin{frame}{Probability interpretations}
    Axioms tell you how to calculate with probabilities.
    % \begin{center}
    %     old probabilities \arrow new probabilities
    % \end{center}
    
    \vspace{0.5cm}
    How do we assign probabilities in the first place? \\
    \arrow Need probability interpretations.

    \vspace{0.5cm}
    The interpretations \textbf{share the same mathematical framework}, but the meaning of $p(x)$ is different.
\end{frame}

\begin{frame}{Frequentist interpretation}
    Assign a probability as relative frequency
    % \begin{minipage}{0.60\textwidth}

        $$ \color{blue}
        p(x) = \lim_{N\to\infty} \frac{N_x}{N}
        $$

        \begin{itemize}
            \item \textbf{Data} is random.
            \item For repeatable experiments only.
            % \item No probabilities for single events.
        \end{itemize}
    % \end{minipage}
    % \begin{minipage}{0.39\textwidth}
    %     \begin{figure}
    %         \centering
    %         \includegraphics[width=\textwidth]{../plots/die-title.png}
    %         \includegraphics[width=\textwidth]{../plots/die-bike.png}
    %         \includegraphics[width=\textwidth]{../plots/die-ski.png}
    %         \includegraphics[width=\textwidth]{../plots/die-climbing.png}
    %     \end{figure}
    %     %https://chessintheair.com/the-risk-of-dying-doing-what-we-love/
    % \end{minipage}
\end{frame}

\begin{frame}{Bayesian interpretation}
    Assign a probability $\color{blue} p(x)$ as \textit{degree of belief}.
    \begin{itemize}
        \item \textbf{Parameters} are random.
        \item Inference results are subjective.
    \end{itemize}
    % $$p(death|experienced) \neq p(death|unexperienced)$$
    % %https://chessintheair.com/the-risk-of-dying-doing-what-we-love/
\end{frame}

\begin{frame}{Bayes' theorem}

The \textbf{posterior} is

$$
\tcbhighmath[colback=yellow]{
p(\text{theory} | \text{data}) = \frac{p(\text{data}|\text{theory}) p(\text{theory})}{p(\text{data})}
}
$$

\begin{itemize}
    \item \textbf{Likelihood} $p(\text{data}|\text{theory})$
    \item \textbf{Prior} $p(\text{theory})$
    \item \textbf{Marginal likelihood} ${p(\text{data}) = \int p(\text{data}|\text{theory}) p(\text{theory})}$
\end{itemize}
\begin{flushright}
    \textit{Can you derive it?}
\end{flushright}
\end{frame}

\begin{frame}
    \textit{ [...] nearly all physicists tend to misinterpret frequentist results as statements about the theory given with the data.}\\
    \flushright \href{https://indico.cern.ch/event/398949/attachments/799330/1095613/The_CLs_Technique.pdf}{A. L. Read}
\end{frame}

\begin{frame}{Bayesian beliefs}
    \begin{minipage}{0.44\textwidth}
        $$\color{blue} p(SM|data)$$
        $$\color{blue} p(Higgs|data)$$
        $$\color{blue} p(SUSY|data)$$
    \end{minipage}
    \onslide<2>
    \begin{minipage}{0.55\textwidth}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{../plots/sm_pred.png}
        \end{figure}
        \small \flushright
        \href{https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PUBNOTES/ATL-PHYS-PUB-2024-011/fig_03a.png}{ATLAS}
    \end{minipage}
\end{frame}

\begin{frame}{Bayesian updating}
    \vspace{-0.5cm}
    Can generally use measurements to \textit{update} our posterior
    $$
    p(\theta | x_1, x_2) 
    = \frac{p(x_2|\theta)}{p(x_2)} \color{red} p(\theta | x_1) \color{black}
    = \frac{p(x_2|\theta)}{p(x_2)} \color{red} \frac{p(x_1|\theta)}{p(x_1)} p(\theta).
    $$
    % Effectively what is done for combining measurement results (eg. \href{https://pdg.lbl.gov/}{PDG averages}).

    \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{../plots/updating.pdf}
    \end{figure}


    % A posterior based on all LHC data $x_{LHC}$
    % $$
    % p(\theta | x_{LHC}) = \frac{p(x_{LHC}|\theta)}{p(x_{LHC})} p(\theta)
    % $$
    % can be updated with LHC-HL data $x_{HL}$, with $p(\theta | x_{LHC})$ as a prior
    % $$
    % p(\theta | x_{LHC}, x_{HL}) = \frac{p(x_{HL}|\theta)}{p(x_{HL})} p(\theta | x_{LHC}) = \frac{p(x_{HL}|\theta)}{p(x_{HL})} \frac{p(x_{LHC}|\theta)}{p(x_{LHC})} p(\theta).
    % $$
    % Parameter values $\theta$ under which data $x_{LHC}, x_{HL}$ is more probable on average get weighted up, other values get weighted down.
\end{frame}

% \begin{frame}{Bayesian updating in real life}
%     \vspace{-0.5cm}
%     PDFs for parameters are manifestly Bayesian.\\
%     Combinations of $\mu \pm \sigma$ assume an underlying PDF for $\mu$.
%     \begin{minipage}{0.44\textwidth}
%         \begin{align*}
%             \mathcal{R}(D) &= \frac{
%                 \mathcal{B}(\bar B \to D \tau^- \bar{\nu}_\tau)
%             }{
%                 \mathcal{B}(\bar B \to D l^- \bar{\nu}_l)
%             }\\
%             \mathcal{R}(D^*) &= \frac{
%                 \mathcal{B}(\bar B \to D^* \tau^- \bar{\nu}_\tau)
%             }{
%                 \mathcal{B}(\bar B \to D^* l^- \bar{\nu}_l)
%             }
%         \end{align*}
%     \end{minipage}
%     \begin{minipage}{0.55\textwidth}
%         \begin{figure}
%             \centering
%             \includegraphics[width=\textwidth]{../plots/rd_rdstar.pdf}
%         \end{figure}
%     \end{minipage}
%     \small Actual combinations a bit more involved \arrow \href{https://arxiv.org/pdf/2411.18639}{HFLAV 2024}.
% \end{frame}

\begin{frame}{The common ground}
    \textbf{Frequentist inference} is based on 
    $$\tcbhighmath[colback=yellow]{p(x|\theta)}$$
    \textbf{Bayesian inference} is based on
    $$p(\theta | x) = \frac{\tcbhighmath[colback=yellow]{p(x|\theta)} p(\theta)}{p(x)}$$
    \centering
    \Large
    \textbf{You want the best possible $\tcbhighmath[colback=yellow]{p(x|\theta)}$!}
\end{frame}

\begin{frame}[noframenumbering]{The best possible model...}
    \begin{figure}
        \includegraphics[width=0.5\textwidth]{../plots/sample_size.jpg}
    \end{figure}
\end{frame}

% \begin{frame}{Physics does not care...}{... about our interpretation}
% For many inference problems, 
% $$\color{blue}
% \text{results} \mid \text{frequentist} \approx \text{results} \mid \text{Bayesian},
% $$
% even though they answer different questions.

% % what are different questions?

% \vspace{0.5cm}

% % BUT if results are different, you should understand why.
% \end{frame}

\begin{frame}
\center
\Large
Parameter inference
\end{frame}

\begin{frame}{Parameter inference}
    \center
    \textbf{Point estimates}\\
    Identify the most probable parameter point.

    \vspace{1cm}

    \textbf{Interval estimation}\\
    Identify extended regions in parameter space based on compatibility with the data.
\end{frame}

\begin{frame}{Frequentist point estimates: estimators}
    Estimator is a \textit{statistic} $\hat \theta(x)$, with desired properties
    \begin{minipage}{0.65\textwidth}
        \begin{itemize}
            \item \textbf{consistency}\\
            $
            \color{blue}
            \lim_{N_x \to \infty} E_x[\hat \theta] = \theta_{true}
            $
            % converges toward true value as number of observations increase
            \item \textbf{unbiasedness}\\
            $
            \color{blue}
            b = E_x[\hat \theta] - \theta_{true}
            $
            
            \item \textbf{efficiency}\\
            $
            \color{blue}
            V(\hat \theta) = I(\theta)^{-1} = E_x\left[\left(\frac{\partial \ln p(x|\theta)}{\partial \theta}\right)^2\right]^{-1}
            $
            % minimum variance
            \item ...
        \end{itemize}
    \end{minipage}
    \begin{minipage}{0.34\textwidth}
        \begin{figure}
            \centering
            \href{https://xkcd.com/}{
            \includegraphics[width=\linewidth]{../plots/selection_bias_2x.png}
            }
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}{Estimator properties}
    \vspace{-0.5cm}
    \begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{../plots/estimator.pdf}
\end{figure}
\begin{flushright}
    \small
    Statistical Methods in Experimental Physics, F. James
\end{flushright}
\end{frame}

\begin{frame}{Method of maximum likelihood}
    \vspace{-0.2cm}
    We find maximum likelihood estimators $ \hat \theta$ by solving
    $$
    \tcbhighmath[colback=yellow]{
        \hat \theta = \text{argmax}_\theta ~ p(x|\theta)
    }
    $$
    It has the property
    $$
    \lim_{N \to \infty} p\left(\sqrt{N}(\hat \theta - \theta_{true})\right) = \mathcal{N}\left(0, I^{-1}(\theta)\right),
    $$
    implying consistency, asymptotic unbiasedness and efficiency.
    % $$ \lim_{N \to \infty} V(\hat \theta) = I(\theta)^{-1} = E\left[\frac{\partial \ln p(x|\theta)}{\partial \theta}\right]^{-1}$$
\end{frame}

% \begin{frame}{Asymptotic normality}
%     In the asymptotic limit $N \to \infty$ they have the properties of
%     $$
%     lim_{N_x \to \infty} \sqrt{N}(\hat \theta - \theta_{true}) \sim \mathcal{N}(0, I^{-1}(\theta))
%     $$
%     \begin{itemize}
%         \item \textit{consistency}
%         \item \textit{efficiency}: variance given by Cramer-Rao bound
%         $$ \lim_{N_x \to \infty} V(\hat \theta) = E\left[\frac{\partial \ln L(x|\theta)}{\partial \theta}\right]^{-1}$$
%         \item \textit{robustness}: asymptotically Normal
%     \end{itemize}
% \end{frame}

\begin{frame}{Bayesian point estimates}
    \begin{minipage}{0.49\textwidth}
        \textbf{Mode}\\ Value of $\theta$ with maximum a-posteriori probability
            $$
            \tcbhighmath[colback=yellow]{
            \theta^* = \text{argmax}_\theta ~ p(\theta|x)
            }
            $$
        \textbf{Mean}\\ Expected value of $\theta$ under the posterior
            $$
            \tcbhighmath[colback=yellow]{ 
            \bar{\theta} = E_{p(\theta|x)}[\theta]
            }
            $$
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \begin{figure}
            \centering
            \includegraphics[width=\linewidth]{../plots/map_vs_mean.pdf}
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}{Bayesian point estimates}
        \textbf{Mode}\\ Value of $\theta$ with maximum a-posteriori probability
            $$\theta^* = \text{argmax}_\theta ~ p(\theta|x)$$
        Stationary point of the posterior at $\theta^*$:
        $$0=\frac{\partial p(\theta|x)}{\partial \theta}\bigg\vert_{\theta = \theta^*} \propto 
        \left(
            \frac{\partial p(x|\theta)}{\partial \theta}p(\theta) + p(x|\theta) \frac{\partial p(\theta)}{\partial \theta}\right)\bigg\vert_{\theta = \theta^*}$$
        $$\implies \theta^* = \hat \theta \quad \text{if} \quad \frac{\partial p(\theta)}{\partial \theta}\bigg\vert_{\theta = \theta^*}=0$$
        Posterior mode and ML estimate agree for \textit{flat} priors.
\end{frame}

\begin{frame}{Intervals and limits}
\begin{figure}
    \centering
    \href{https://xkcd.com/}{
    \includegraphics[width=0.25\linewidth]{../plots/confidence_interval.png}
    }
\end{figure}
\end{frame}

\begin{frame}{Non-Normal PDFs}
For non-normal estimator PDFs, $\hat \theta \pm \sigma_\theta$ can be misleading.

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{../plots/gamma.pdf}
\end{figure}
\end{frame}

\begin{frame}{Frequentist \textit{confidence} intervals}

\begin{minipage}[t]{0.55\linewidth}
\textbf{Neyman confidence belt}
$$
\tcbhighmath[colback=yellow]{
\int_{x_1}^{x_2} dx ~ p(x|\theta) = 1-\alpha
}
$$
Not unique \arrow \textit{central interval}
\small
$$
\int_{-\infty}^{x_1} dx ~ p(x|\theta)=  \int_{x_2}^{\infty} dx ~ p(x|\theta) = \alpha/2
$$
\normalsize
or \textit{upper/lower interval}
\end{minipage}
\begin{minipage}[t]{0.44\linewidth}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{../plots/neyman.png}
    \end{figure}
\end{minipage}
\end{frame}

\begin{frame}{Bayesian \textit{credible} intervals}

Credible intervals (CI) $[\theta_1, \theta_2]$ cover $1-\alpha$ of the posterior
\begin{minipage}{0.49\linewidth}
    $$
    \tcbhighmath[colback=yellow]{
    \int_{\theta_1}^{\theta_2} d\theta ~ p(\theta| x) = 1-\alpha
    }
    $$
    \begin{itemize}
        \item Upper/lower CI
        \item Highest (posterior) density intervals (HDI)
    \end{itemize}
\end{minipage}
\begin{minipage}{0.49\linewidth}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{../plots/intervals.pdf}
    \end{figure}
\end{minipage}

\end{frame}

\begin{frame}{$b\to u l^- \bar \nu$ in the Weak Effective Theory}
    \vspace{-0.5cm}
    \begin{minipage}{0.5\textwidth}
        \begin{figure}
            \includegraphics[width=\textwidth]{../plots/wet-posterior.png}
        \end{figure}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \begin{itemize}
            \item \href{https://corner.readthedocs.io/en/latest/}{Corner plots} are great for visualization.
            \item Posterior for Wilson coefficients
            \item Identify modes, credible intervals
        \end{itemize}
        \vspace{0.5cm}
        \begin{flushright}
            \small
            \href{https://arxiv.org/pdf/2302.05268}{arXiv:2302.05268v2 [hep-ph]}
        \end{flushright}
    \end{minipage}
\end{frame}

\begin{frame}
    \center
    \Large
    Nuisance parameters and priors
    \end{frame}
    
    \begin{frame}{Nuisance parameters}

        \begin{minipage}{0.49\textwidth}
            Models are not perfect \\
            \arrow \textbf{systematic bias}

            \vspace{0.5cm}
            \textit{Solution}: \\
            \textbf{Nuisance} parameters $\nu$,
            $$p(x|\psi, \nu)$$
        \end{minipage}
        \begin{minipage}{0.49\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{../plots/hubble.png}
            \end{figure}
            \flushright \small
            \href{https://www.pnas.org/doi/10.1073/pnas.15.3.168}{Hubble 1929}
        \end{minipage}
    \vspace{0.5cm}
    % In general the model is not perfect, which is to say it cannot provide an accurate description of the data even at the most optimal point of its parameter space. As a result, the estimated parameters can have a systematic bias.
    
    % Although including additional parameters may eliminate or at least reduce the effect of systematic uncertainties, their presence will result in increased statistical uncertainties for the parameters of interest. This occurs because the estimators for the nuisance parameters and those of interest will in general be correlated.
    
    Usually, we want to constrain nuisance parameters, but in the frequentist language \textbf{everything is data}.
    % To reduce the impact of the nuisance parameters one often tries to constrain their values by means of control or calibration measurements, say, having data y.
    
\end{frame}
    
    % \begin{frame}{... everything is data}
    %     "\textit{The great advantage of the Bayesian approach is that it allows you to incorporate subjective beliefs, while the Frequentist approach pretends that you don't have any.}"
    %     \flushright -- associated with Jim Berger by ChatGPT
    % \end{frame}
    
    % \begin{frame}{... everything is data}
    %     {\centering Are we being honest here?}
    
    %     Prior knowledge in a typical frequentist analysis
    %     \begin{itemize}
    %         \item theory predictions
    %         \item model parameters
    %         \item missing higher-order corrections
    %         \item MC normalizations
    %         \item ...
    %     \end{itemize}
    %     % https://indico.cern.ch/event/243641/attachments/415317/577061/CERN-Stat-Lectures.pdf
    % \end{frame}
    
\begin{frame}{Frequentist "priors"}
    \vspace{-0.5cm}
    \begin{minipage}{0.7\textwidth}
        Constrain nuisance parameters using \textit{auxiliary data} $a$,
        $$p(x| \psi, \nu) p(a| \nu)$$
        
        Often \textit{auxiliary data} is \textit{created} to match our desired constraint term.

        \vspace{0.5cm}

        $p(a| \nu)$ represents \textit{degree of belief} in $\nu$.

        \flushright \small
        \href{https://arxiv.org/pdf/2311.14647}{Belle II 2024}
    \end{minipage}
    \begin{minipage}{0.29\textwidth}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{../plots/knunu-signal.pdf}
            \includegraphics[width=\textwidth]{../plots/knunu-offres.pdf}
        \end{figure}
    \end{minipage}
    \end{frame}
    
\begin{frame}{Bayesian nuisance parameters}
    \begin{minipage}{0.6\textwidth}
        Priors can be defined using auxiliary data
        $$p(\nu|a) \propto p(a|\nu) p_0(\nu)$$
        %If $p_0(\nu)$ is chosen to have minimal impact, this overlaps with the frequentist treatment.
        
        Only in the Bayesian case, other prior choices are also allowed
        $$ p(\nu) = \mathcal{N}(\nu | \nu_0, \sigma_\nu^2)$$
        
        % Bayesian approach also requires priors for POIs.
    \end{minipage}
    \begin{minipage}{0.39\textwidth}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{../plots/straub-posterior.pdf}
            \flushright \small
        \href{https://arxiv.org/pdf/2311.14647}{Paul 2017}
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}{A simple linear model}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{../plots/change_in_slope_2x.png}
    \end{figure}
    % https://www.explainxkcd.com/wiki/images/9/9d/change_in_slope_2x.png
\end{frame}

\begin{frame}{A simple linear model}
    % https://www.pp.rhul.ac.uk/~cowan/stat/beijing10/cowan_beijing10_5.pdf
    % https://pdg.lbl.gov/2024/reviews/rpp2024-rev-statistics.pdf

    \begin{itemize}
        \item Independent data : $\boldsymbol{X} = (x_i, y_i, \sigma_i)$
    \end{itemize}

    \begin{figure}
        \centering
        \includegraphics[width=0.5\linewidth]{../plots/linear_data.pdf}
    \end{figure}

\end{frame}

\begin{frame}{A simple linear model}
    \begin{itemize}
        \item Independent data: $\boldsymbol{X} = (x_i, y_i, \sigma_i)$
        \item[\arrow] Model = product of normal distributions:
        $$ p(\boldsymbol{X}|\alpha, \beta) = \prod_{x_i, y_i,\sigma_i \in \boldsymbol{X}}\mathcal{N}(y_i | \mu(x_i|\alpha, \beta), \sigma_i^2)$$
        $$\mu(x_i|\alpha, \beta) = \alpha + \beta x_i$$
         \item We want to know about $\alpha$, do not care about $\beta$.
    \end{itemize}
\end{frame}

\begin{frame}{Frequentist analysis}
\vspace{-1cm}
    $$ -2\log p(\boldsymbol{X}|\alpha, \beta) = \sum_{x_i, y_i,\sigma_i \in \boldsymbol{X}}\frac{\left(y_i -\mu(x_i|\alpha, \beta)\right)^2}{\sigma_i^2}$$
    \begin{figure}
        \centering
        \includegraphics[width=0.49\linewidth]{../plots/nll_unconstr.pdf}
    \end{figure}
\end{frame}

\begin{frame}{Including a measurement of $\beta$: $t, \sigma_{t}$}
\vspace{-1cm}
    $$ -2\log p(\boldsymbol{X}|\alpha, \beta) = \sum_{x_i, y_i,\sigma_i \in \boldsymbol{X}}\frac{\left(y_i -\mu(x_i|\alpha, \beta)\right)^2}{\sigma_i^2} \color{red} + \frac{\left(\beta -t\right)^2}{\sigma_{t}^2} $$
    \begin{minipage}{0.49\textwidth}
        \begin{figure}
            \centering
            unconstrained
            \includegraphics[width=0.9\linewidth]{../plots/nll_unconstr.pdf}
        \end{figure}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \begin{figure}
            \centering
            constrained
            \includegraphics[width=0.9\linewidth]{../plots/nll_constr.pdf}
        \end{figure}
    \end{minipage}

\end{frame}

\begin{frame}{Posterior}
\vspace{-1cm}
    $$p(\alpha, \beta|\boldsymbol{X}) \propto p(\boldsymbol{X}|\alpha, \beta) p(\alpha)p(\beta)$$
    \begin{minipage}{0.4\linewidth}
        $$p(\alpha) = \text{Uniform}(0,2)$$
        $$p(\beta) = \mathcal{N}(\beta | t_1, \sigma_{t_1}^2)$$
    \end{minipage}
    \begin{minipage}{0.59\linewidth}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{../plots/posterior.pdf}
    \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}{Marginal posterior}
\vspace{-1cm}
    $$p(\alpha|\boldsymbol{X}) = \int d\beta ~ p(\alpha, \beta|\boldsymbol{X}) = \mathcal{N}(\alpha | \alpha^*, \sigma_{\alpha})$$
    \begin{minipage}{0.4\linewidth}
    In this example, we get
        \begin{itemize}
            \item $\alpha^* = \hat \alpha$
            % $$
            % \hat \alpha =
            % $$
            \item 68\% HDI = $\hat \alpha \pm \sigma_{\alpha}$
            % $$
            % \sigma_{\alpha}^2 = \sum_{x_i,  \sigma_i\in \boldsymbol{x}, \boldsymbol{\sigma}}\sigma_{t_1}x_i^2 + \sigma_i^2
            % $$
        \end{itemize}
    \end{minipage}
    \begin{minipage}{0.59\linewidth}
    \begin{figure}
        \centering
        \includegraphics[width=0.7\linewidth]{../plots/marginal_posterior.pdf}
    \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}{How do we marginalize?}
    \begin{itemize}
        \item We only want the posterior for $\psi$ alone.
        \item Remove nuisance parameters by integrating over $\nu$.
        \item[\arrow] The \textit{marginal posterior} is
            $$
            p(\psi | x) = \int d\nu ~ p(\psi, \nu | x)
            $$
        \item Commonly a high dimensional integral\\ \arrow compute with Monte Carlo methods.
    \end{itemize}
\end{frame}

\begin{frame}
\center
\Large
Markov Chain Monte Carlo (MCMC)
\end{frame}

\begin{frame}{Markov chain}
    A sequence of events, where probability of the next state depends solely on the current state
    $$
    \ldots \to \theta_{i} \sim g(\theta_{i} | \theta_{i-1}) \to \theta_{i+1} \sim g(\theta_{i+1} | \theta_{i}) \to \ldots
    $$
    for some \textit{proposal distribution} $g$. 
\end{frame}
\begin{frame}{MCMC integration}
    \begin{figure}
        \centering
        \includegraphics[width=0.6\linewidth]{../plots/mcmc_diagram.png}
    \end{figure}
    % https://doi.org/10.3390/app10010272
\end{frame}

\begin{frame}{Metropolis-Hastings}
We loop
\begin{enumerate}
    \item Generate $\theta \sim g(\theta|\theta_i)$
    \item Update
    \begin{equation*}
        \theta_{i+1} =
        \begin{cases}
            \theta  &\quad u \leq \text{min}\left(1, \frac{p(\theta)g(\theta|\theta_i)}{p(\theta_i)g(\theta_i|\theta)}\right)\\
            \theta_i &\quad \text{otherwise}
        \end{cases}
    \end{equation*}
    where $u \sim \text{Uniform}(0, 1)$
\end{enumerate}
\textit{Note}: for example $g(\theta|\theta_0) = \mathcal{N}(\theta|\theta_0, \sigma)$.
\end{frame}

\begin{frame}{Chains}
    In MCMC we generate a sequence
    $$
    \theta_0 \to \theta_1 \to \theta_2 \to \ldots
    $$
    \begin{minipage}{0.49\textwidth}
        Only one start can land you a in local minima.
        \begin{align*}
            &\theta_0^0 \to \theta_1^0 \to \theta_2^0 \to \ldots \\
            &\theta_0^1 \to \theta_1^1 \to \theta_2^1 \to \ldots \\
            &\theta_0^2 \to \theta_1^2 \to \theta_2^2 \to \ldots \\
            &\ldots
        \end{align*}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{../plots/chain_walk.pdf}
        \end{figure}
    \end{minipage}

\end{frame}

\begin{frame}{Convergence}
    Trace plots are a useful convergence diagnostic
    \begin{minipage}[t]{0.49\linewidth}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{../plots/trace_theta0.pdf}
    \end{figure}
    \end{minipage}
    \begin{minipage}[t]{0.49\linewidth}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{../plots/trace_theta1.pdf}
    \end{figure}
    \end{minipage}
    ... but one can become more fancy. %TODO reference
\end{frame}

\begin{frame}
    \center
    \Large
    Model comparison
\end{frame}

\begin{frame}{Frequentist: P-values}
    \vspace{-1cm}
    $$
    P(\lambda_{obs}|M_0) = \int_{\lambda_{obs}}^\infty d\lambda ~ p(\lambda|M_0), \quad  \lambda = -2 \ln \frac{p(x|\hat \theta_0, M_0)}{p(x|\hat \theta_1, M_1)} ~^\dagger
    $$
    \begin{figure}
        \centering
        \includegraphics[width=0.65\textwidth]{../plots/hypo.pdf}
    \end{figure}
    \small
    $^\dagger$ Likelihood ratio = optimal test statistic \arrow Newman-Pearson lemma
\end{frame}

\begin{frame}{Averaged: Bayes factor}
    \vspace{-0.5cm}
        \begin{minipage}{0.74\textwidth}
            Compare the probabilities of the observed data being produced by a given model.
            \begin{align*}
                p(\theta | x, M) &= \frac{p(x | \theta, M) ~ p(\theta | M)}{\textcolor{red}{p(x | M)}}\\
                \textcolor{red}{p(x | M)} &= \int d^n \theta ~ p(x | \theta, M) ~ p(\theta | M)\\
                B &= \frac{p(x | M_1)}{p(x| M_0)}
            \end{align*}
            \textit{Do you see a potential hazard?}
        \end{minipage}
        \begin{minipage}{0.25\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{../plots/jeffreys-scale.png}
                %https://www.researchgate.net/publication/341958289/figure/fig1/AS:902030868615168@1592072326255/JASP-classification-scheme-for-the-Bayes-factor-BF-10.png
            \end{figure}
        \end{minipage}
    \end{frame}

\begin{frame}{$b\to u l^- \bar \nu$ in the Weak Effective Theory}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{../plots/wet-bayes.png}
    \end{figure}
    $$
    B = \exp(\ln Z(WET) - \ln Z (SM)) = 54.6
    $$
    \begin{flushright}
        \href{https://arxiv.org/pdf/2302.05268}{arXiv:2302.05268v2 [hep-ph]}
    \end{flushright}
\end{frame}

\begin{frame}{Tools to try}
\begin{figure}
    \center
    \href{
        https://github.com/bat
    }{\includegraphics[width=0.3\textwidth]{../plots/bat.pdf}}
    \hfill
    \href{
        https://www.pymc.io/
    }{\includegraphics[width=0.35\textwidth]{../plots/pymc.png}}
    \hfill
    \href{
        https://python.arviz.org/en/stable/
    }{\includegraphics[width=0.3\textwidth]{../plots/arviz.png}}
\end{figure}
\end{frame}

% \begin{frame}{Exercises}

% \textbf{a simple MCMC algorithm}\\

% \end{frame}

% \begin{frame}{Exercises: s+b problem}
% \small
% You do a binned analysis. You observe [155, 121,  13] in each bin respectively. From simulation, you expect to see [90, 30, 0] signal events (where the last bin acts as a control measurement) and [50, 70, 10] background events.\\
% Hint: It is better to work with log-probabilities
% \end{frame}

% \begin{frame}{Exercises: s+b problem}
% \small
% \begin{itemize}
%     \item Construct a likelihood function (chi2)
%     \item What is the best fit point?
%     \item What are the frequentist limits?
%     \item What would a sensible choice of priors be for this example?
%     \item Use an implementation of the Metropolis Hastings algorithm to sample from the posterior
%     \item Make a corner plot of the posterior
%     \item How do the mode of the posterior and the credible intervals compare to the best fit point and frequentist limits?
% \end{itemize}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix




\end{document}


% https://drive.google.com/drive/folders/16CIMfhQkyEqMYkhLcsnzN0Yu5C2WHNU74

% https://pdg.lbl.gov/2024/reviews/rpp2024-rev-statistics.pdf

interested in using a given sample of data to make inferences about a probabilistic model

In Bayesian statistics, the subjective interpretation of probability is used to quantify one’s
degree of belief in a hypothesis. This allows one to define a probability density function (p.d.f.) for
a parameter, which reflects one’s knowledge about where its true value lies.

 hypothesis H is a statement about the probability for the data,
often written P (x|H)

If the probability P (x|H) for data x is regarded as a function of the hypothesis H, then it is
called the likelihood of H, usually written L(H). Often the hypothesis is characterized by one or
more parameters θ, in which case L(θ) = P (x|θ) is called the likelihood function.

In the Bayesian approach, inference is based on the posterior probability for H given the data
x, which represents one’s degree of belief that H is true given the data. 

Bayesian statistics supplies no unique rule for determining the prior.  it is important to carry out a sensitivity analysis, that is, to show how the result changes under a reasonable variation of the prior probabilities

For the special case of a constant prior, one can see from Bayes’ theorem (40.37) that the
posterior is proportional to the likelihood, and therefore the mode (peak position) of the posterior
is equal to the maximum-likelihood estimator. The posterior mode, however, will change in general
upon a transformation of parameter. One may use as the Bayesian estimator a summary statistic
other than the mode, such as the median, which is invariant under parameter transformation. But
this will not in general coincide with the MLE